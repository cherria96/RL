Args in experiment:
Namespace(random_seed=100, is_training=1, model_id='Patchformer_pretrain_sbk_7_1_22_28_11', model='PatchFormer_pretrain', data='sbk', root_path='./data/', data_path='sbk_ad_selected.csv', features='MS', target='BP', freq='d', checkpoints='./checkpoints/', n_subs=2, seq_len=28, label_len=7, pred_len=11, series_decomposition=True, kernel_size=7, decoder_mode='future_subs', individual=False, fc_dropout=0.1, head_dropout=0.0, patch_len=7, stride=1, padding_patch='end', affine=0, subtract_last=0, embed_type=0, enc_in=11, dec_in=11, c_out=11, d_model=32, n_heads=4, e_layers=3, d_layers=3, d_ff=128, moving_avg=3, factor=1, distil=True, dropout=0.05, embed='fixed', activation='gelu', output_attention=True, do_predict=False, revin=True, wodenorm=False, win_size=100, step=1, optimizer='adam', alpha=0.5, beta=0.5, dp_rank=8, rescale=1, merge_size=2, momentum=0.1, local_rank=0, devices_number=1, use_statistic=False, use_decomp=False, same_smoothing=False, warmup_epochs=0, num_workers=10, itr=1, train_epochs=50, batch_size=32, patience=3, learning_rate=0.001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False)
Use GPU: cuda:0
>>>>>>>start training : Patchformer_pretrain_sbk_7_1_22_28_11_PatchFormer_pretrain_sbk_ftMS_sl28_ll7_pl11_dm32_nh4_el3_dl3_df128_fc1_ebfixed_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 2032
val 652
test 652
Epoch: 1 cost time: 1.3852097988128662
Epoch: 1, Steps: 63 | Train Loss: 0.9506694 Vali Loss: 1.8525164 Test Loss: 1.4767535
Validation loss decreased (inf --> 1.852516).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 1.1791753768920898
Epoch: 2, Steps: 63 | Train Loss: 0.8104341 Vali Loss: 1.7638872 Test Loss: 1.3926775
Validation loss decreased (1.852516 --> 1.763887).  Saving model ...
Updating learning rate to 0.001
Epoch: 3 cost time: 1.3753280639648438
Epoch: 3, Steps: 63 | Train Loss: 0.6505331 Vali Loss: 2.0240331 Test Loss: 1.2949895
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.001
Epoch: 4 cost time: 1.2494115829467773
Epoch: 4, Steps: 63 | Train Loss: 0.5860117 Vali Loss: 1.9497429 Test Loss: 1.2232227
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0009000000000000001
Epoch: 5 cost time: 1.2603790760040283
Epoch: 5, Steps: 63 | Train Loss: 0.5097842 Vali Loss: 1.6845583 Test Loss: 1.2135603
Validation loss decreased (1.763887 --> 1.684558).  Saving model ...
Updating learning rate to 0.0008100000000000001
Epoch: 6 cost time: 1.1603569984436035
Epoch: 6, Steps: 63 | Train Loss: 0.4595698 Vali Loss: 2.0936697 Test Loss: 1.1924441
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0007290000000000002
Epoch: 7 cost time: 1.5049207210540771
Epoch: 7, Steps: 63 | Train Loss: 0.4266781 Vali Loss: 1.8975284 Test Loss: 1.1600239
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0006561000000000001
Epoch: 8 cost time: 1.341933012008667
Epoch: 8, Steps: 63 | Train Loss: 0.3859489 Vali Loss: 1.5756135 Test Loss: 1.2172691
Validation loss decreased (1.684558 --> 1.575613).  Saving model ...
Updating learning rate to 0.00059049
Epoch: 9 cost time: 1.4155645370483398
Epoch: 9, Steps: 63 | Train Loss: 0.3763422 Vali Loss: 1.6551697 Test Loss: 1.2103882
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.000531441
Epoch: 10 cost time: 1.2999496459960938
Epoch: 10, Steps: 63 | Train Loss: 0.3606189 Vali Loss: 1.5553222 Test Loss: 1.1222727
Validation loss decreased (1.575613 --> 1.555322).  Saving model ...
Updating learning rate to 0.0004782969000000001
Epoch: 11 cost time: 1.2960503101348877
Epoch: 11, Steps: 63 | Train Loss: 0.3459714 Vali Loss: 1.4540795 Test Loss: 1.0983613
Validation loss decreased (1.555322 --> 1.454080).  Saving model ...
Updating learning rate to 0.0004304672100000001
Epoch: 12 cost time: 1.3583488464355469
Epoch: 12, Steps: 63 | Train Loss: 0.3338905 Vali Loss: 1.6069863 Test Loss: 1.1245617
EarlyStopping counter: 1 out of 3
Updating learning rate to 0.0003874204890000001
Epoch: 13 cost time: 1.4231204986572266
Epoch: 13, Steps: 63 | Train Loss: 0.3234953 Vali Loss: 1.5717106 Test Loss: 1.0928026
EarlyStopping counter: 2 out of 3
Updating learning rate to 0.0003486784401000001
Epoch: 14 cost time: 1.3269851207733154
Epoch: 14, Steps: 63 | Train Loss: 0.3153156 Vali Loss: 1.5915467 Test Loss: 1.1117607
EarlyStopping counter: 3 out of 3
Early stopping
Total Training Time: 28.18 seconds
Total Trainable Parameters: 107,247
>>>>>>>testing : Patchformer_pretrain_sbk_7_1_22_28_11_PatchFormer_pretrain_sbk_ftMS_sl28_ll7_pl11_dm32_nh4_el3_dl3_df128_fc1_ebfixed_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 652
